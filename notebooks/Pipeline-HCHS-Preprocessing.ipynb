{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21288fd",
   "metadata": {},
   "source": [
    "# Mediapipe preprocessing\n",
    "\n",
    "#### This notebook contains the mediapipe functions to detect and crop relevant regions from unsorted photographs. Please specify a folder (INPUT_FOLDER) which contains subfolders; one for each subject. The subfolders shall contain unsorted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac797569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"D:/derma2/\"  # <-- these are the unsorted images\n",
    "SAVE_DIR = \"D:/testoutput/\"  # <-- crops will be saved here\n",
    "\n",
    "PLOT = False  # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e32712",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = glob(INPUT_FOLDER + \"/*/*.JPG\")\n",
    "print(\"Individual images:\", len(all_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc5a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "def vec_length(v: np.array):\n",
    "    return np.sqrt(sum(i**2 for i in v))\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def dotproduct(v1, v2):\n",
    "    return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "    return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "def angle(v1, v2):\n",
    "    return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "    \n",
    "def find_head(image):\n",
    "    with mp_face_mesh.FaceMesh(min_detection_confidence=0.7, min_tracking_confidence=0.5) as face_mesh:\n",
    "        drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "        \n",
    "        results = face_mesh.process(image)\n",
    "    return results\n",
    "    \n",
    "mp_pose = mp.solutions.pose    \n",
    "def find_body(image):\n",
    "    \n",
    "    with mp_pose.Pose(static_image_mode=True, model_complexity=2, \n",
    "                      enable_segmentation=True, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(image)\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_landmarks(results, image):\n",
    "    size, color = 100, \"red\"\n",
    "    \n",
    "    left_hüfte = results.pose_landmarks.landmark[23]\n",
    "    right_hüfte = results.pose_landmarks.landmark[24]\n",
    "    orientation_vektor =(np.array([left_hüfte.x, left_hüfte.y, left_hüfte.z]) - np.array([right_hüfte.x, right_hüfte.y, right_hüfte.z]))\n",
    "\n",
    "    a = angle(orientation_vektor, [1, 0, 0])\n",
    "    b = angle(orientation_vektor, [0, 0, 1])\n",
    "    c = angle(orientation_vektor, [-1, 0, 0])\n",
    "    d = angle(orientation_vektor, [0, 0, -1])\n",
    "   # print(\"Landmarks found at position:\", np.argmin([a,b,c,d]), \n",
    "   #       list([\"front\", \"right\", \"back\", \"left\"])[np.argmin([a,b,c,d])])\n",
    "    \n",
    "    for f in range(32):\n",
    "        if(f in [23,24,12]):\n",
    "            size, color = 100, \"red\"\n",
    "        else:\n",
    "            size, color = 30, \"yellow\"\n",
    "\n",
    "        x = results.pose_landmarks.landmark[f].x * image.shape[1]\n",
    "        y = results.pose_landmarks.landmark[f].y * image.shape[0]\n",
    "        if PLOT:\n",
    "            circle1 = plt.Circle((x, y), size, color=color)\n",
    "            plt.gca().add_patch(circle1)\n",
    "    if PLOT:\n",
    "        plt.imshow(image)\n",
    "        plt.show()   \n",
    "    \n",
    "    orientation = [\"front\", \"right\", \"back\", \"left\"][np.argmin([a,b,c,d])]\n",
    "    hipsInPicture = True\n",
    "    headInPicture = True\n",
    "    for f in range(32):\n",
    "        if results.pose_landmarks.landmark[f].y > 1.2: hipsInPicture = False\n",
    "    for f in range(32):\n",
    "        if results.pose_landmarks.landmark[f].y < 0.05: headInPicture = False\n",
    "    \n",
    "    ys = []\n",
    "    xs = []\n",
    "    for f in range(32):\n",
    "        ys.append(results.pose_landmarks.landmark[f].y * image.shape[0])\n",
    "        xs.append(results.pose_landmarks.landmark[f].x * image.shape[1])\n",
    "   \n",
    "    x1, y1, x2, y2 = int(np.min(xs)), int(np.min(ys)), int(np.max(xs)), int(np.max(ys))\n",
    "    vv = abs(y1-y2) \n",
    "    uu = abs(x2-x1)\n",
    "    cropareaBody = [int(max(x1 - uu*0.1,0)), \n",
    "                    int(max(y1 - vv*0.02,0)), \n",
    "                    int(min(x2 + uu*0.1, image.shape[1])), \n",
    "                    int(min(y2 + vv*0.02, image.shape[0]))]\n",
    "    \n",
    "    return {\"orientation\": orientation, \"lowerlimb_inpicture\": hipsInPicture, \n",
    "            \"head_inpicture\": headInPicture, \"crop_area_body\": cropareaBody}\n",
    "\n",
    "\n",
    "def extract_face(results, image):\n",
    "    annotated_image = image.copy()\n",
    "    if results.multi_face_landmarks is not None:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for f in range(468):          \n",
    "                if(np.isin(f, [10, 137, 152, 366])):\n",
    "                    color = \"red\"\n",
    "                    size = 50\n",
    "                else:\n",
    "                    color = \"yellow\"\n",
    "                    size = 10\n",
    "                if PLOT:\n",
    "                    circle1 = plt.Circle((results.multi_face_landmarks[0].landmark[f].x * image.shape[1], results.multi_face_landmarks[0].landmark[f].y * image.shape[0]), size, color=color)\n",
    "                    plt.gca().add_patch(circle1)\n",
    "                    \n",
    "        _x = results.multi_face_landmarks[0].landmark[10].x * image.shape[1]\n",
    "        _y = results.multi_face_landmarks[0].landmark[10].y * image.shape[0]\n",
    "        _x2 = results.multi_face_landmarks[0].landmark[152].x * image.shape[1]\n",
    "        _y2 = results.multi_face_landmarks[0].landmark[152].y * image.shape[0]\n",
    "        vector1 = np.array([_x, _y]) - np.array([_x2,_y2])\n",
    "        if PLOT:\n",
    "            plt.plot([_x,_x2], [_y,_y2], label=\"v1\")\n",
    "        \n",
    "        _x = results.multi_face_landmarks[0].landmark[10].x * image.shape[1]\n",
    "        _y = results.multi_face_landmarks[0].landmark[10].y * image.shape[0]\n",
    "        _x2 = results.multi_face_landmarks[0].landmark[137].x * image.shape[1]\n",
    "        _y2 = results.multi_face_landmarks[0].landmark[137].y * image.shape[0]\n",
    "        vector2 = np.array([_x, _y]) - np.array([_x2,_y2])\n",
    "        if PLOT:\n",
    "            plt.plot([_x,_x2], [_y,_y2], label=\"v2\")\n",
    "        \n",
    "        _x = results.multi_face_landmarks[0].landmark[10].x * image.shape[1]\n",
    "        _y = results.multi_face_landmarks[0].landmark[10].y * image.shape[0]\n",
    "        _x2 = results.multi_face_landmarks[0].landmark[366].x * image.shape[1]\n",
    "        _y2 = results.multi_face_landmarks[0].landmark[366].y * image.shape[0]\n",
    "        vector3 = np.array([_x, _y]) - np.array([_x2,_y2])\n",
    "        if PLOT:\n",
    "            plt.plot([_x,_x2], [_y,_y2], label=\"v3\")\n",
    "        \n",
    "        \n",
    "        myDict = {\"headTurned\": False}\n",
    "        if min(angle(vector1, vector2), angle(vector1, vector3)) < 0.45:\n",
    "            myDict[\"headTurned\"] = True\n",
    "        #print(\"angle\", angle(vector1, vector2), angle(vector1, vector3))\n",
    "        \n",
    "        if PLOT:\n",
    "            #plt.plot([_x,_x2], [_y,_y2])\n",
    "            plt.imshow(image)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "        x1 = (results.multi_face_landmarks[0].landmark[137].x * image.shape[1])\n",
    "        y1 = (results.multi_face_landmarks[0].landmark[10].y * image.shape[0])\n",
    "        x2 = (results.multi_face_landmarks[0].landmark[366].x * image.shape[1])\n",
    "        y2 = (results.multi_face_landmarks[0].landmark[152].y * image.shape[0])\n",
    "        w = x2-x1\n",
    "        h = y2-y1\n",
    "        x1 = np.maximum(0,x1-(w*0.2))\n",
    "        x2 = np.minimum(image.shape[1],x2+(w*0.2))\n",
    "        y1 = np.maximum(0,y1-(h*0.3))\n",
    "        y2 = np.minimum(image.shape[0],y2+(h*0.2))\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        myDict[\"crop_area_face\"] = [x1, y1, x2, y2]  # todo: hier noch augen landmark mit rein\n",
    "        \n",
    "        \n",
    "\n",
    "        return myDict\n",
    "\n",
    "#https://google.github.io/mediapipe/solutions/pose\n",
    "\n",
    "def stats_to_label(stats):\n",
    "    assert type(stats) == dict\n",
    "    \n",
    "    if stats[\"hasBody\"] and not stats[\"hasFace\"]: # nur körper\n",
    "        if stats[\"lowerlimb_inpicture\"]:\n",
    "            if stats[\"head_inpicture\"] == False:\n",
    "                if stats[\"orientation\"] == \"right\":  return \"body right\"\n",
    "                elif stats[\"orientation\"] == \"front\": return \"body front\"\n",
    "                elif stats[\"orientation\"] == \"back\":  return \"body back\"\n",
    "                elif stats[\"orientation\"] == \"left\":  return \"body left\"\n",
    "                else: print(\"error. not impl\")\n",
    "            else:\n",
    "                print(\"bend over\")\n",
    "        else:\n",
    "            return \"face side or back\"\n",
    "            \n",
    "    if stats[\"hasFace\"] and stats[\"hasBody\"]: # gesicht und körper\n",
    "        if stats[\"lowerlimb_inpicture\"] == True: print(\"body and face\") \n",
    "        else:\n",
    "            if stats[\"orientation\"] == \"front\": \n",
    "                if stats[\"headTurned\"] == False:\n",
    "                    return \"face front\"\n",
    "                else:\n",
    "                    print(\"has face, has body, oriented to front, head turned slightly\")\n",
    "            else: print(\"Error. No implementation for\", stats[\"orientation\"])\n",
    "    \n",
    "    if stats[\"hasFace\"] and not stats[\"hasBody\"]:\n",
    "        if stats[\"headTurned\"] == False:\n",
    "            return \"face front\"\n",
    "        else:\n",
    "            print(\"Error not implementeddd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3796bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(im, fill=0):\n",
    "    long = np.max(im.shape[0:2])\n",
    "    short = np.min(im.shape[0:2])\n",
    "    if len(im.shape) == 3:\n",
    "        if im.shape[0] < im.shape[1]:\n",
    "            im=np.pad(im,(((long-short)//2,(long-short)//2),(0,0), (0,0)), constant_values=fill)\n",
    "        else:\n",
    "            im = np.pad(im, ((0,0), ((long-short)//2,(long-short)//2), (0,0)), constant_values=fill)\n",
    "    else:\n",
    "        if im.shape[0] < im.shape[1]:\n",
    "            im=np.pad(im,(((long-short)//2,(long-short)//2),(0,0)), constant_values=fill)\n",
    "        else:\n",
    "            im = np.pad(im, ((0,0), ((long-short)//2,(long-short)//2)), constant_values=fill)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_label(k):\n",
    "    k = k.replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    #print(\"__\"*50)\n",
    "    #print(\"Processing\", k)\n",
    "\n",
    "    img = cv2.imread(k)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    \n",
    "    # run mediapipe\n",
    "    results_facedet = find_head(img)\n",
    "    results_body = find_body(img)\n",
    "    \n",
    "    stats = {\"hasBody\":  results_body.pose_landmarks is not None, \n",
    "             \"hasFace\": results_facedet.multi_face_landmarks is not None}\n",
    "\n",
    "    # append face statistics\n",
    "    if stats[\"hasFace\"]:\n",
    "        stats_face = extract_face(results_facedet, img)\n",
    "        for key in stats_face.keys():\n",
    "            stats[key] = stats_face[key]\n",
    "    \n",
    "    # append body statistics\n",
    "    if stats[\"hasBody\"]:\n",
    "        stats_body = extract_landmarks(results_body, img)\n",
    "        for key in stats_body.keys():\n",
    "            stats[key] = stats_body[key]\n",
    "\n",
    "    label = stats_to_label(stats)\n",
    "    print(\"label:\", label)\n",
    "    #print(stats)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    ####### safe crops for face and body back/front#######\n",
    "    ######################################################\n",
    "    \n",
    "    patientfolder = SAVE_DIR + \"/\" + k.split(\"/\")[2] + \"/\"\n",
    "    ps = k.split(\"/\")[3]\n",
    "    def try_make_folder(patientfolder):\n",
    "        try:\n",
    "            os.mkdir(patientfolder)\n",
    "        except:\n",
    "            print(\"folder already exists:\", patientfolder)\n",
    "    \n",
    "    try:\n",
    "        if \"crop_area_face\" in stats.keys() and label == \"face front\":\n",
    "            try_make_folder(patientfolder)\n",
    "            x1, y1, x2, y2 = stats[\"crop_area_face\"]  # todo: hier das neue übergeben (siehe oben) und als csv mitsaven\n",
    "            tosave = pad_to_square(cv2.cvtColor(img[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "            tosave = cv2.resize(tosave, (1028, 1028))\n",
    "            cv2.imwrite(patientfolder + f\"/face_{ps}.jpg\", tosave)\n",
    "            \n",
    "\n",
    "        elif label in [\"body back\"] and \"crop_area_body\" in stats.keys():\n",
    "            try_make_folder(patientfolder)\n",
    "            x1, y1, x2, y2 = stats[\"crop_area_body\"]\n",
    "            tosave = pad_to_square(cv2.cvtColor(img[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "            tosave = cv2.resize(tosave, (1028, 1028))\n",
    "            cv2.imwrite(patientfolder + f\"/back_{ps}.jpg\", tosave)\n",
    "\n",
    "        elif label in [\"body front\"] and \"crop_area_body\" in stats.keys():\n",
    "            try_make_folder(patientfolder)\n",
    "            x1, y1, x2, y2 = stats[\"crop_area_body\"]\n",
    "            tosave = pad_to_square(cv2.cvtColor(img[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "            tosave = cv2.resize(tosave, (1028, 1028))\n",
    "            cv2.imwrite(patientfolder + f\"/front_{ps}.jpg\", tosave)\n",
    "    except:\n",
    "        print(\"error\", k)\n",
    "\n",
    "    #if PLOT:\n",
    "    #    if label in [\"body front\", \"body \"]\n",
    "    #    plt.imshow(img[y1:y2, x1:x2])\n",
    "    #    plt.axis(\"off\")\n",
    "    #    plt.show()\n",
    "        \n",
    "        \n",
    "    return [k, k.split(\"/\")[2], label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f772d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "results = Parallel(n_jobs=16, verbose=0, backend=\"threading\", timeout=10)(\n",
    "             map(delayed(detect_label), tqdm(all_imgs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb9d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1dc70d",
   "metadata": {},
   "source": [
    "## Save face keypoint information\n",
    "#### Only store the facial keypoints for facedrop analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"D:/mediapipe/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc02f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "    \n",
    "def find_head(image):\n",
    "    with mp_face_mesh.FaceMesh(min_detection_confidence=0.7, min_tracking_confidence=0.5) as face_mesh:\n",
    "        drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "        \n",
    "        results = face_mesh.process(image)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob(SAVE_DIR+\"/*/face*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_faces_imgs = glob.glob(SAVE_DIR+\"/*/face*.jpg\")\n",
    "\n",
    "def save_face_landmark_numpy(pth):\n",
    "    im = cv2.imread(pth)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    results = find_head(im)\n",
    "    if results.multi_face_landmarks is None:\n",
    "        os.remove(pth)\n",
    "        print(\"Removed file\", pth)\n",
    "        return\n",
    "    face_points = np.array([(results.multi_face_landmarks[0].landmark[i].x, \n",
    "      results.multi_face_landmarks[0].landmark[i].y) for i in range(468)])\n",
    "    np.save(pth.split(\".\")[0]+\".npy\", face_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6324805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "results = Parallel(n_jobs=16, verbose=0, backend=\"threading\", timeout=10)(\n",
    "             map(delayed(save_face_landmark_numpy), tqdm(all_faces_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(pth)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "results = find_head(im)\n",
    "\n",
    "face_points = np.array([(results.multi_face_landmarks[0].landmark[i].x *im.shape[1], \n",
    "  results.multi_face_landmarks[0].landmark[i].y *im.shape[0]) for i in range(468)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(im)\n",
    "plt.scatter(face_points.T[0], face_points.T[1])\n",
    "for i in range(468):\n",
    "    plt.annotate(str(i), (face_points.T[0][i], face_points.T[1][i]), color=\"r\", size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08c223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = np.zeros_like(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_right = [107,66,105,63,70,156,35,31,228,229,230,231,232,128,245,193,55]\n",
    "eye_left = [336, 296, 334, 293, 300, 383, 265, 261, 448, 449, 450, 451, 452, 357, 465, 417, 285]\n",
    "nase = [151, 336, 285, 417, 465, 343, 277, 355, 429, 358, 327, 326, 2, 97,98,129, 209, 126, 47, 114, 245, 193, 55, 107]\n",
    "mund = [0,267,269,270,409,287,273,335,406,313,18,83,182,106,43,57,185,40,39,37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a93ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = face_points[eye_right].astype(int)\n",
    "cv2.fillPoly(template, pts = [contours], color =(255,0,0))\n",
    "\n",
    "contours = face_points[eye_left].astype(int)\n",
    "cv2.fillPoly(template, pts = [contours], color =(255,0,0))\n",
    "\n",
    "contours = face_points[nase].astype(int)\n",
    "cv2.fillPoly(template, pts = [contours], color =(255,255,0))\n",
    "\n",
    "contours = face_points[mund].astype(int)\n",
    "cv2.fillPoly(template, pts = [contours], color =(255,0,255))\n",
    "\n",
    "plt.imshow(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bf481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac88dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cc07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
